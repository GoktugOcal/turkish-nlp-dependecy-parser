{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59e25218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import conllu\n",
    "import zeyrek\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f548dacf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "APPENDING RESULT: <(kale_Noun)(-)(kale:noun_S + a3sg_S + m:p1sg_S + nom_ST)>\n",
      "APPENDING RESULT: <(Kale_Noun_Prop)(-)(kale:nounProper_S + a3sg_S + m:p1sg_S + nom_ST)>\n",
      "APPENDING RESULT: <(kalem_Noun)(-)(kalem:noun_S + a3sg_S + pnon_S + nom_ST)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Parse(word='kalem', lemma='kale', pos='Noun', morphemes=['Noun', 'A3sg', 'P1sg'], formatted='[kale:Noun] kale:Noun+A3sg+m:P1sg'),\n",
       " Parse(word='kalem', lemma='Kale', pos='Noun', morphemes=['Noun', 'A3sg', 'P1sg'], formatted='[Kale:Noun,Prop] kale:Noun+A3sg+m:P1sg'),\n",
       " Parse(word='kalem', lemma='kalem', pos='Noun', morphemes=['Noun', 'A3sg'], formatted='[kalem:Noun] kalem:Noun+A3sg')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer = zeyrek.MorphAnalyzer()\n",
    "analyzer.analyze(\"kalemin\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3164b9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "APPENDING RESULT: <(okumak_Verb)(-)(ok:verbRoot_VowelDrop_S + uyor:vProgYor_S + du:vPastAfterTense_S + lar:vA3pl_ST)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Parse(word='okuyordular', lemma='okumak', pos='Verb', morphemes=['Verb', 'Prog1', 'Past', 'A3pl'], formatted='[okumak:Verb] ok:Verb+uyor:Prog1+du:Past+lar:A3pl')]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse = analyzer.analyze(\"okuyordular\")[0]\n",
    "parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a113cfb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "APPENDING RESULT: <(Roman_Noun_Prop)(-)(roman:nounProper_S + a3sg_S + ları:p3pl_S + nom_ST)>\n",
      "APPENDING RESULT: <(Roman_Noun_Prop)(-)(roman:nounProper_S + lar:a3pl_S + pnon_S + ı:acc_ST)>\n",
      "APPENDING RESULT: <(Roman_Noun_Prop)(-)(roman:nounProper_S + lar:a3pl_S + ı:p3sg_S + nom_ST)>\n",
      "APPENDING RESULT: <(Roman_Noun_Prop)(-)(roman:nounProper_S + lar:a3pl_S + ı:p3pl_S + nom_ST)>\n",
      "APPENDING RESULT: <(roman_Noun)(-)(roman:noun_S + a3sg_S + ları:p3pl_S + nom_ST)>\n",
      "APPENDING RESULT: <(roman_Noun)(-)(roman:noun_S + lar:a3pl_S + pnon_S + ı:acc_ST)>\n",
      "APPENDING RESULT: <(roman_Noun)(-)(roman:noun_S + lar:a3pl_S + ı:p3sg_S + nom_ST)>\n",
      "APPENDING RESULT: <(roman_Noun)(-)(roman:noun_S + lar:a3pl_S + ı:p3pl_S + nom_ST)>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun P3pl None\n",
      "Noun Acc None\n",
      "Noun P3sg None\n",
      "Noun P3pl None\n",
      "Noun P3pl None\n",
      "Noun Acc None\n",
      "Noun P3sg None\n",
      "Noun P3pl None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['NP3PL', 'NP3', 'NP']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_time(morph_list):\n",
    "    time = None\n",
    "    if \"Prog1\" in morph_list: time =  \"Present1\"\n",
    "    if \"Prog2\" in morph_list: time =  \"Present2\"\n",
    "    if \"Fut\" in morph_list: time =  \"Fut\"\n",
    "    if \"Past\" in morph_list: time =  \"Past\"\n",
    "    return time\n",
    "\n",
    "def word_parse(token):\n",
    "    parses = analyzer.analyze(token)[0]\n",
    "    possible = []\n",
    "    for parse in parses:\n",
    "        pos = parse.pos\n",
    "        suff = parse.morphemes[-1]\n",
    "        time = get_time(parse.morphemes)\n",
    "        print(pos, suff, time)\n",
    "        if pos == \"Verb\":\n",
    "            if time != None:\n",
    "                nonterminal = \"VP\"\n",
    "                if \"Past\" in time: nonterminal += \"PAST\"\n",
    "                elif \"Present\" in time: nonterminal += \"PRE\"\n",
    "                elif \"Fut\" in time: nonterminal += \"FUT\"\n",
    "\n",
    "                if suff == \"A1pl\": nonterminal += \"1PL\"\n",
    "                elif suff == \"A1sg\": nonterminal += \"1\"\n",
    "                elif suff == \"A2pl\": nonterminal += \"2PL\"\n",
    "                elif suff == \"A2sg\": nonterminal += \"2\"\n",
    "                elif suff == \"A3pl\": nonterminal += \"3PL\"\n",
    "                elif suff == \"A3sg\": nonterminal += \"3\"\n",
    "\n",
    "                possible.append(nonterminal)\n",
    "            else: continue\n",
    "            \n",
    "        elif pos == \"Noun\":\n",
    "            nonterminal = \"NP\"\n",
    "            if \"1pl\" in suff: nonterminal += \"1PL\"\n",
    "            elif \"1sg\" in suff: nonterminal += \"1\"\n",
    "            elif \"2pl\" in suff: nonterminal += \"2PL\"\n",
    "            elif \"2sg\" in suff: nonterminal += \"2\"\n",
    "            elif \"3pl\" in suff: nonterminal += \"3PL\"\n",
    "            elif \"3sg\" in suff: nonterminal += \"3\"\n",
    "                \n",
    "            possible.append(nonterminal)\n",
    "            \n",
    "        else: possible.append(pos.upper())\n",
    "            \n",
    "    return possible\n",
    "        \n",
    "list(set(word_parse(\"romanları\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8496c04d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A3pl'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse.morphemes[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "222554cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Prog1\" in parse.morphemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00858117",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "APPENDING RESULT: <(yüz_Num_Card)(-)(yüz:numeralRoot_ST)>\n",
      "APPENDING RESULT: <(yüzmek_Verb)(-)(yüz:verbRoot_S + vImp_S + vA2sg_ST)>\n",
      "APPENDING RESULT: <(yüz_Noun)(-)(yüz:noun_S + a3sg_S + pnon_S + nom_ST)>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num - Num\n",
      "Verb - A2sg\n",
      "Noun - A3sg\n"
     ]
    }
   ],
   "source": [
    "for parse in analyzer.analyze(\"yüz\")[0]:\n",
    "    print(parse.pos, \"-\", parse.morphemes[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3f00bdc5",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MorphAnalyzer' object has no attribute 'suffix_parse'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43manalyzer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuffix_parse\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mokuyorum\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MorphAnalyzer' object has no attribute 'suffix_parse'"
     ]
    }
   ],
   "source": [
    "analyzer.suffix_parse(\"okuyorum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d3223f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e95c1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ea485f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de84eee7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f03c63f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c89c4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af1b19d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7079aabc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3431fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_raw = open('data/corpus/UD_Turkish-BOUN/tr_boun-ud-train.conllu', 'r', encoding=\"utf-8\").read()\n",
    "data_raw = open('data/corpus/UD_Turkish-Penn/tr_penn-ud-train.conllu', 'r', encoding=\"utf-8\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39db9b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = conllu.parse(data_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7ab68688",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "uposes = []\n",
    "for sent in sentences:\n",
    "    for token in sent:\n",
    "        tokens.append(token[\"form\"])\n",
    "        uposes.append(token[\"upos\"])\n",
    "        \n",
    "lexicon = {}\n",
    "stems = set()\n",
    "for sent in sentences:\n",
    "    for token in sent:\n",
    "        upos = token[\"upos\"]\n",
    "        if upos in lexicon.keys(): lexicon[upos].add(token[\"lemma\"].lower())\n",
    "        else: lexicon[upos] = set([token[\"lemma\"].lower()])\n",
    "        stems.add(token[\"lemma\"].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ccd3bea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(166514, 166514)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens), len(uposes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "8a222f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ADJ',\n",
       " 'ADP',\n",
       " 'ADV',\n",
       " 'AUX',\n",
       " 'CCONJ',\n",
       " 'DET',\n",
       " 'INTJ',\n",
       " 'NOUN',\n",
       " 'NUM',\n",
       " 'PRON',\n",
       " 'PROPN',\n",
       " 'PUNCT',\n",
       " 'SCONJ',\n",
       " 'VERB',\n",
       " 'X'}"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(uposes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "e6be046c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sydney',\n",
       " 'maxicare',\n",
       " 'aldous',\n",
       " \"airlines'ın\",\n",
       " 'garcia',\n",
       " 'edelson',\n",
       " 'spinney',\n",
       " 'martha',\n",
       " 'jefferies',\n",
       " 'j.',\n",
       " 'nehri',\n",
       " 'kennedy',\n",
       " 'orr',\n",
       " 'bear',\n",
       " 'bloch',\n",
       " 'technology',\n",
       " 'provident',\n",
       " \"bnl'ın\",\n",
       " 'gasb',\n",
       " 'components',\n",
       " 'virginia',\n",
       " 'ara',\n",
       " 'sultan',\n",
       " 'squibb',\n",
       " 'mickey',\n",
       " 'hughey',\n",
       " 'kennametal',\n",
       " 'emc',\n",
       " 'co.',\n",
       " 'malpass',\n",
       " 'aruba',\n",
       " 'thompson-csf',\n",
       " 'excel',\n",
       " 'filenet',\n",
       " 'gortari',\n",
       " 'matthews',\n",
       " 'llerena',\n",
       " 'amex',\n",
       " 'natwest',\n",
       " 'pictures',\n",
       " 'daniel',\n",
       " 'andrew',\n",
       " \"lincoln'e\",\n",
       " 'eurostat',\n",
       " 'bern',\n",
       " 'out',\n",
       " 'aeroflot',\n",
       " \"close'nun\",\n",
       " 'popular',\n",
       " 'ıdeal',\n",
       " 'airborne',\n",
       " 'torres',\n",
       " 'foundation',\n",
       " 'deniz',\n",
       " 'yaşam',\n",
       " 'heidi',\n",
       " 'ragu',\n",
       " 'prego',\n",
       " 'nicholas',\n",
       " 'rima',\n",
       " 'sparc',\n",
       " 'office',\n",
       " 'bronner',\n",
       " 'northern',\n",
       " 'wilfred',\n",
       " 'kaiser',\n",
       " 'nacion',\n",
       " 'heynow',\n",
       " 'ıbm',\n",
       " 'greenspan',\n",
       " 'dunes',\n",
       " 'comfort',\n",
       " 'shell',\n",
       " 'lexus',\n",
       " 'education',\n",
       " 'phelan',\n",
       " \"westmoreland'ın\",\n",
       " 'heineken',\n",
       " 'garman',\n",
       " 'bat',\n",
       " 'rorer',\n",
       " 'conway',\n",
       " 'morrison',\n",
       " 'edison',\n",
       " 'comair',\n",
       " \"nuggets'ın\",\n",
       " 'homestake',\n",
       " 'hammack',\n",
       " 'brinks',\n",
       " 'private',\n",
       " 'charles',\n",
       " 'bizmart',\n",
       " 'brezilya',\n",
       " 'halife',\n",
       " 'evelyn',\n",
       " 'brady',\n",
       " 'polymerix',\n",
       " 'petit',\n",
       " 'ferroelektrik',\n",
       " \"rafale'e\",\n",
       " 'fibercom',\n",
       " 'democratic',\n",
       " 'algiers',\n",
       " 'reina',\n",
       " 'contel',\n",
       " 'gelbart',\n",
       " 'feng-hsiung',\n",
       " 'yuzek',\n",
       " 'gm',\n",
       " \"dna'sı\",\n",
       " 'kosta',\n",
       " 'redford',\n",
       " 'elianti',\n",
       " 'hewlett',\n",
       " 'rockefeller',\n",
       " 'teksaslı',\n",
       " 'fiat',\n",
       " 'hells',\n",
       " 'kaliforniya',\n",
       " 'bankers',\n",
       " 'abd-filipin',\n",
       " 'runkel',\n",
       " 'cri',\n",
       " \"worth'de\",\n",
       " 'torontolu',\n",
       " 'keating',\n",
       " 'harper',\n",
       " 'webb',\n",
       " 'chugai',\n",
       " 'dominici',\n",
       " 'caterpillar',\n",
       " 'k.',\n",
       " 'nec',\n",
       " 'economist',\n",
       " 'tucson',\n",
       " 'fha-sigortalı',\n",
       " 'days',\n",
       " 'hanedan',\n",
       " 'smurfit',\n",
       " 'hitachi',\n",
       " 'ugi',\n",
       " 'kloves',\n",
       " 'conn.-merkezli',\n",
       " 'shakespeare',\n",
       " 'tvx',\n",
       " 'memory',\n",
       " 'shaw',\n",
       " 'boise',\n",
       " 'benefit',\n",
       " 'nevada',\n",
       " 'wildbad',\n",
       " 'channel',\n",
       " 'schaefer',\n",
       " 'mixte',\n",
       " \"simmons'ın\",\n",
       " 'kamboçyalı',\n",
       " 'calgary',\n",
       " 'palmero',\n",
       " 'crossland',\n",
       " 'moody',\n",
       " 'işletme',\n",
       " 'a.c.',\n",
       " 'grace',\n",
       " 'elliott',\n",
       " 'vaclav',\n",
       " 'norton',\n",
       " 'people',\n",
       " 'a.',\n",
       " 'ackerman',\n",
       " 'microsystems',\n",
       " 'dryja',\n",
       " 'darman',\n",
       " 'bileşik',\n",
       " 'investment',\n",
       " 'ti',\n",
       " 'cantobank',\n",
       " 'family',\n",
       " 'rawls',\n",
       " 'benny',\n",
       " 'pignatelli',\n",
       " 'ahmad',\n",
       " 'litchfield',\n",
       " 'r',\n",
       " 'marathon',\n",
       " 'ducky',\n",
       " 'louis',\n",
       " 'çinli',\n",
       " 'arnold',\n",
       " 'starr',\n",
       " 'mission',\n",
       " 'hashidate',\n",
       " 'jaguar-gm',\n",
       " 'gibson',\n",
       " 'lbo',\n",
       " 'belli',\n",
       " 'norman',\n",
       " 'robinson',\n",
       " 'moore',\n",
       " \"gsmh'nın\",\n",
       " 'vax',\n",
       " 'l.j.',\n",
       " 'international',\n",
       " 'brierley',\n",
       " \"casey'nin\",\n",
       " 'control',\n",
       " 'jos.',\n",
       " 'felipe',\n",
       " 'matamoros',\n",
       " 'clements',\n",
       " 'hardee',\n",
       " 'gelnbart',\n",
       " \"hole'da\",\n",
       " \"coats'un\",\n",
       " 'amsterdam',\n",
       " 'linda',\n",
       " 'canada',\n",
       " 'kerr-mcgee',\n",
       " 'telecommunications',\n",
       " 'cottrell',\n",
       " 'uaw',\n",
       " 'kyle',\n",
       " 'traviata',\n",
       " 'abd-japonya',\n",
       " 'cimri',\n",
       " 'boon',\n",
       " 'u',\n",
       " 'comerica',\n",
       " 'best',\n",
       " 'department',\n",
       " 'london',\n",
       " 'viatech',\n",
       " 'minpeco',\n",
       " \"maybelline'ın\",\n",
       " 'smale',\n",
       " \"at&t'nin\",\n",
       " 'emshwiller',\n",
       " 'gabriela',\n",
       " 'batı',\n",
       " 'nasa',\n",
       " 'movieline',\n",
       " 'transmission',\n",
       " \"corp.'dan\",\n",
       " 'mehl',\n",
       " 'perlman',\n",
       " 'gray',\n",
       " 'salomon',\n",
       " 'mit',\n",
       " 'broadway',\n",
       " 'shops',\n",
       " 'crown',\n",
       " 'boxes',\n",
       " 'heimz',\n",
       " 'molière',\n",
       " 'est,',\n",
       " 'bell',\n",
       " 'andre',\n",
       " 'amr',\n",
       " 'laidlaw',\n",
       " \"tandy'nin\",\n",
       " 'korando',\n",
       " 'the',\n",
       " 'louis-dreyfus',\n",
       " 'ventes',\n",
       " 'wilcock',\n",
       " 'brick',\n",
       " 'barney',\n",
       " 'sun',\n",
       " 'commerzbank',\n",
       " 'gate',\n",
       " 'güneybatı',\n",
       " \"krisher'ın\",\n",
       " 'biosciences',\n",
       " 'avustralyalı',\n",
       " 'ındustries',\n",
       " 'inco',\n",
       " 'semel',\n",
       " 'lyondell',\n",
       " 'gorbaçov',\n",
       " 'bruno',\n",
       " 'jaguar',\n",
       " \"b'gosh\",\n",
       " 'spider',\n",
       " 'seita',\n",
       " 'rubens',\n",
       " 'labonte',\n",
       " 'mcguire',\n",
       " 'santa',\n",
       " 'dornan',\n",
       " 'conner',\n",
       " 'fkö',\n",
       " 'next',\n",
       " 'sidhpur',\n",
       " 'newmont',\n",
       " 'kote',\n",
       " 'toronto-merkezli',\n",
       " 'macmillan',\n",
       " 'maria',\n",
       " 'bullocks',\n",
       " 'cruz',\n",
       " 'waterbury',\n",
       " 'dodd',\n",
       " 'dellums',\n",
       " 'ldp',\n",
       " 'superfund',\n",
       " 'port',\n",
       " 'a.b.d',\n",
       " 'fletcher',\n",
       " 'bahar',\n",
       " 'gardolaplarının',\n",
       " 'fruit',\n",
       " 'interstate',\n",
       " 'lowenthal',\n",
       " 'lipper',\n",
       " 'rita',\n",
       " 'kurumu',\n",
       " 'lawson',\n",
       " 'castro-medellin',\n",
       " 'manila',\n",
       " 'tech',\n",
       " 'hammacklarla',\n",
       " 'fukuyama',\n",
       " \"technologies'den\",\n",
       " 'comex',\n",
       " 'mistake',\n",
       " 'başkan',\n",
       " 'gamble',\n",
       " 'merck',\n",
       " 'arrow',\n",
       " 'rockford',\n",
       " 'accident',\n",
       " 'sihanouk',\n",
       " 'teleflora',\n",
       " 'polycast',\n",
       " 'anahattını',\n",
       " 'hixson',\n",
       " 'singapor',\n",
       " 'hovnanian',\n",
       " 'triton',\n",
       " 'nissan',\n",
       " 'robie',\n",
       " 'christie',\n",
       " 'akre',\n",
       " 'viroqua',\n",
       " 'grenfell',\n",
       " 'power',\n",
       " 'glenn',\n",
       " 'mccall',\n",
       " 'pae',\n",
       " 'braniff',\n",
       " 'is',\n",
       " 'toy‘un',\n",
       " 'mort',\n",
       " \"quebecor'ın\",\n",
       " 'aqua',\n",
       " 'schlumberger',\n",
       " 'four',\n",
       " 'o.',\n",
       " 'oneida',\n",
       " 'freeberg',\n",
       " 'cyanamid',\n",
       " 'lorraine',\n",
       " 'luther',\n",
       " 'air',\n",
       " 'hollanda',\n",
       " 'stone',\n",
       " 'tom',\n",
       " 'dan',\n",
       " 'hyman',\n",
       " 'venezuela',\n",
       " 'balcor',\n",
       " \"georgia-pacific'ten\",\n",
       " 'demokratlar',\n",
       " 'g',\n",
       " 'tanrı',\n",
       " 'development',\n",
       " 'shannon',\n",
       " 'willard',\n",
       " 'composite',\n",
       " 'yale',\n",
       " 'motoren',\n",
       " 'tad',\n",
       " 'ana',\n",
       " 'easter',\n",
       " 'banxquote',\n",
       " 'malcolm',\n",
       " 'trinity',\n",
       " 'cantor',\n",
       " 'station',\n",
       " 'kossuth',\n",
       " 'kenneth',\n",
       " \"o'donnell\",\n",
       " 'fastball',\n",
       " \"usx'ın\",\n",
       " 'lubkin',\n",
       " 'boys',\n",
       " 'brussels',\n",
       " 'va.',\n",
       " 'terry',\n",
       " 'kidder',\n",
       " 'northeast',\n",
       " 'marsam',\n",
       " 'ferranti',\n",
       " 'harsco',\n",
       " 'greens',\n",
       " \"angeles'ta\",\n",
       " 'walsh',\n",
       " 'seib',\n",
       " 'investments',\n",
       " 'dempsey',\n",
       " 'response',\n",
       " 'leiby',\n",
       " 'markese',\n",
       " 'jeb',\n",
       " 'chiron',\n",
       " 'windsor',\n",
       " \"o'dwyer\",\n",
       " 'ben',\n",
       " 'alcee',\n",
       " 'campo',\n",
       " 'randy',\n",
       " \"williams'ın\",\n",
       " 'kasparov',\n",
       " 'erc',\n",
       " 'kleinwort',\n",
       " 'cadillac',\n",
       " 'herman',\n",
       " 'hooker',\n",
       " 'steinberg',\n",
       " 'cum.',\n",
       " 'matagorda',\n",
       " 'fidelity',\n",
       " 'mason',\n",
       " 'tim',\n",
       " 'edwin',\n",
       " 'gramm-rudman',\n",
       " \"fcb\\\\/leber'e\",\n",
       " 'beach',\n",
       " 'mitre',\n",
       " 'paragon',\n",
       " 'grauer',\n",
       " 'banponce',\n",
       " 'selkin',\n",
       " 'kantorei',\n",
       " 'anheuser-busch',\n",
       " 'whitman',\n",
       " 'picocassette',\n",
       " 'd.t.',\n",
       " 'alliance',\n",
       " 'sands',\n",
       " 'armco',\n",
       " 'mezzogiorno',\n",
       " 'grossman',\n",
       " 'komunist',\n",
       " 's.',\n",
       " 'anatol',\n",
       " \"arby's'in\",\n",
       " 'samuel',\n",
       " 'finks',\n",
       " 'sembol:hrb',\n",
       " 'ılluminating',\n",
       " 'honduraslı',\n",
       " 'ashland',\n",
       " 'woodward',\n",
       " 'frankel',\n",
       " 'horta',\n",
       " 'sen.',\n",
       " 'prix',\n",
       " 'machines',\n",
       " 'bmw',\n",
       " 'duston',\n",
       " 'asman',\n",
       " 'siena',\n",
       " 'metatrace',\n",
       " 'purnick',\n",
       " 'cbs',\n",
       " 'kaplan',\n",
       " 'helga',\n",
       " 'schering',\n",
       " 'erkek',\n",
       " \"bronx'ta\",\n",
       " \"eagle'in\",\n",
       " 'mottram',\n",
       " 'toussie',\n",
       " 'embarcadero',\n",
       " 'misawa',\n",
       " 'brozman',\n",
       " 'merrill',\n",
       " 'anne',\n",
       " 'kotobuki',\n",
       " 'hockney',\n",
       " 'shlaes',\n",
       " 'm&a',\n",
       " 'tagg',\n",
       " 'clinic',\n",
       " 'doyle',\n",
       " 'lidgerwood',\n",
       " 'commerciale',\n",
       " 'kenn',\n",
       " 'first',\n",
       " 'jeff',\n",
       " 'ulusal',\n",
       " 'foy',\n",
       " 'smith-kline',\n",
       " 'vranian',\n",
       " 'petrol',\n",
       " 'co.,',\n",
       " 'jude',\n",
       " 'saab-scania',\n",
       " 'vermont',\n",
       " 'realist',\n",
       " 'bicycle',\n",
       " 'newport',\n",
       " 'afgan',\n",
       " 'nrm',\n",
       " 'renaissance',\n",
       " 'dudley',\n",
       " 'ev-senato',\n",
       " 'varşova',\n",
       " 'lease',\n",
       " 'dingell',\n",
       " 'boccone',\n",
       " 'de',\n",
       " 'ivy',\n",
       " 'tilly',\n",
       " 'timber',\n",
       " 'b.a.t.',\n",
       " 'lines',\n",
       " 'stuart',\n",
       " 'tv',\n",
       " 'morgenzon',\n",
       " \"whitbread'ın\",\n",
       " 'digital',\n",
       " \"warner-lambert'in\",\n",
       " 'strieber',\n",
       " 'kurul',\n",
       " 'arty',\n",
       " \"loggia'ın\",\n",
       " 'public',\n",
       " 'orioles',\n",
       " 'danny',\n",
       " 'weichern',\n",
       " 'levin',\n",
       " 'income',\n",
       " 'suisse',\n",
       " 'mother',\n",
       " 'violetta',\n",
       " 'henning',\n",
       " 'hang',\n",
       " 'watergate',\n",
       " 'kleinman',\n",
       " 'stuttgart',\n",
       " 'cane',\n",
       " 'beale',\n",
       " 'bağımlılık-arıtma',\n",
       " 'pagong',\n",
       " \"dictaphone'a\",\n",
       " 'hava',\n",
       " 'kerry',\n",
       " 'olin',\n",
       " 'gillette',\n",
       " 'bartlett',\n",
       " 'roe',\n",
       " 'hunterdon',\n",
       " 'anacomp',\n",
       " 'city',\n",
       " 'bayan.',\n",
       " 'volokh',\n",
       " 'carolica',\n",
       " 'provigo',\n",
       " 'schmidlin',\n",
       " 'electronics',\n",
       " 'hart-scott-rodino',\n",
       " 'ridley',\n",
       " 'raul',\n",
       " 'sioux',\n",
       " 'hines',\n",
       " 'boudreau',\n",
       " 'exterior',\n",
       " 'çin',\n",
       " 'dönüş',\n",
       " 'mathewson',\n",
       " 'henri',\n",
       " 'cae',\n",
       " 'brookings',\n",
       " 'exploration',\n",
       " 'b.v.,',\n",
       " 'david',\n",
       " 'bengladeş',\n",
       " 'sawyer',\n",
       " 'macaristan',\n",
       " 'feinman',\n",
       " \"burton'dı\",\n",
       " 's.a',\n",
       " 'dpc',\n",
       " 'spartalılarla',\n",
       " 'salk',\n",
       " 'swift',\n",
       " 'eidsmo',\n",
       " 'galax',\n",
       " 'color',\n",
       " 'nissho',\n",
       " \"inc.'i\",\n",
       " 'blum',\n",
       " 'franklin',\n",
       " 'usx',\n",
       " 'lalonde',\n",
       " 'werner',\n",
       " 'malpraktis',\n",
       " 'bussieres',\n",
       " 'schroder',\n",
       " 'fannie',\n",
       " 'şeker',\n",
       " 'unilever',\n",
       " \"pbs'te\",\n",
       " 'siemienas',\n",
       " 'matra',\n",
       " 'antonini',\n",
       " 'engelken',\n",
       " 'urban',\n",
       " 'laurence',\n",
       " 'learning',\n",
       " 'g.m.b',\n",
       " 'missouri',\n",
       " 'fullerton',\n",
       " \"rosen'a\",\n",
       " 'chambers',\n",
       " 'mcclelland',\n",
       " 'kohut',\n",
       " 'humana',\n",
       " 'salinas',\n",
       " 'avustralya',\n",
       " 'swiss',\n",
       " 'barry',\n",
       " 'leish',\n",
       " 'julian',\n",
       " 'haven',\n",
       " 'standard',\n",
       " 'tie-vole-ee',\n",
       " 'morton',\n",
       " 'mcgowan',\n",
       " 'kredi-destekli',\n",
       " 'canton',\n",
       " 'minneapolis',\n",
       " 'milt',\n",
       " 'faktoring',\n",
       " 'containers',\n",
       " \"welles'in\",\n",
       " 'ventura',\n",
       " 'john',\n",
       " 'carlucci',\n",
       " 'nasqad',\n",
       " 'kartal',\n",
       " 'morris',\n",
       " \"prudential-bache'dan\",\n",
       " 'board',\n",
       " 'retinablastoma',\n",
       " 'kilpatrick',\n",
       " 'nellcor',\n",
       " 'ordu',\n",
       " \"alexander'a\",\n",
       " 'furukawa',\n",
       " 'sperry',\n",
       " 'jelinski',\n",
       " 'hâli̇ne',\n",
       " 'honda',\n",
       " \"ibm'e\",\n",
       " 'kâr',\n",
       " 'tokyoda',\n",
       " 'bethlehem',\n",
       " \"roberts'ın\",\n",
       " 'indian',\n",
       " 'sciences',\n",
       " 'albion',\n",
       " 'haskins',\n",
       " \"woodstream'in\",\n",
       " 'blok',\n",
       " 'shioya',\n",
       " 'line',\n",
       " 'ıntegrated',\n",
       " 'northwest',\n",
       " 'parkshore',\n",
       " 'jefferson',\n",
       " 'hollinger',\n",
       " 'winnebago',\n",
       " 'dave',\n",
       " 'phillips',\n",
       " \"gmac'te\",\n",
       " 'garratt',\n",
       " 'kaliforniyalı',\n",
       " 'yönet',\n",
       " 'thompson',\n",
       " 'stromeyer',\n",
       " 'fda',\n",
       " 'kia',\n",
       " 'micro',\n",
       " 'meksika-amerika',\n",
       " 'tezhah',\n",
       " 'raskolnikov',\n",
       " \"d'amico\",\n",
       " 'nolan',\n",
       " 'jules',\n",
       " 'nebraska',\n",
       " 'espectador',\n",
       " 'coin',\n",
       " \"nynex'teki\",\n",
       " 'warburg',\n",
       " 'hallucigenia',\n",
       " 'ltd.',\n",
       " 'jerell',\n",
       " 'sony',\n",
       " 'lipstein',\n",
       " 'nazer',\n",
       " 'bilzerian',\n",
       " 'sabha',\n",
       " 'oxford',\n",
       " 'ntt',\n",
       " 'sherwin-williams',\n",
       " 'sea',\n",
       " 'cascade',\n",
       " 'fbı',\n",
       " \"stone'a\",\n",
       " 'hunt',\n",
       " 'devices',\n",
       " 'c.',\n",
       " 'd.c.',\n",
       " 'rubicam',\n",
       " \"time'ın\",\n",
       " 'trecker',\n",
       " 'darby',\n",
       " 'nationwide',\n",
       " 'd.',\n",
       " 'i̇ndianapolis',\n",
       " 'arkansas',\n",
       " 'z.',\n",
       " 'storage',\n",
       " 'sayın',\n",
       " \"at&t'ye\",\n",
       " 'whitford',\n",
       " 'gatward',\n",
       " 'quebecor',\n",
       " 'atkinson',\n",
       " 'petco',\n",
       " \"heller/breene'yi\",\n",
       " 'occupational-urgent',\n",
       " 'camilo',\n",
       " 'barn',\n",
       " 'güney',\n",
       " 'leigh-pemberton',\n",
       " 'procter',\n",
       " 'topaz',\n",
       " 'saab',\n",
       " 'resources',\n",
       " 'bogart',\n",
       " 'association',\n",
       " 'tele-communications',\n",
       " \"vogelstein'ın\",\n",
       " 'maxwell',\n",
       " 'monako',\n",
       " 'unix',\n",
       " 'joanne',\n",
       " 'ventures',\n",
       " 'seagate',\n",
       " 'buffalo',\n",
       " 'orange',\n",
       " \"breeder's\",\n",
       " 'seifert',\n",
       " 'supermarket',\n",
       " 'prizm',\n",
       " 'jelenic',\n",
       " \"chase'in\",\n",
       " 'lübnan',\n",
       " 'johnny',\n",
       " 'scott',\n",
       " 'electronic',\n",
       " 'umman',\n",
       " 'monte',\n",
       " 'johnstown',\n",
       " 'ceo',\n",
       " 'jordan',\n",
       " \"wyoming'e\",\n",
       " 'mexico',\n",
       " 'hemweg',\n",
       " 'france',\n",
       " 'ohbayashi',\n",
       " 'jenrette',\n",
       " 'fournier',\n",
       " 'sassy',\n",
       " 'hyundai',\n",
       " 'employers',\n",
       " 'aguirre-sacasa',\n",
       " 'dresdner',\n",
       " 'healthcare',\n",
       " \"deposit'ın\",\n",
       " 'antarktika',\n",
       " 'aa',\n",
       " 'dinkins',\n",
       " 'high',\n",
       " 'price',\n",
       " 'hibor',\n",
       " 'philip',\n",
       " 'a.g.',\n",
       " \"a.ş'ni\",\n",
       " 'duff',\n",
       " 'bakım',\n",
       " 'loewi',\n",
       " 'basf',\n",
       " 'dereyfus',\n",
       " 'renault',\n",
       " 'stokely',\n",
       " 'arabistan',\n",
       " 'inwood',\n",
       " 'simpson',\n",
       " 'hint',\n",
       " 'toni',\n",
       " 'delchamps',\n",
       " 'tracer',\n",
       " 'alpharetta',\n",
       " 'hong',\n",
       " 'pilgrim',\n",
       " 'pennsylvania',\n",
       " 'asyalı',\n",
       " 'kimmel',\n",
       " 'cano',\n",
       " 'sdı',\n",
       " 'literary',\n",
       " 'tana',\n",
       " 'amerada',\n",
       " 'dean',\n",
       " 'britton',\n",
       " 'hughes',\n",
       " 'mines',\n",
       " 'slater',\n",
       " 'eward',\n",
       " 'sermaye-mal',\n",
       " 'view',\n",
       " 'banking',\n",
       " 'foret',\n",
       " 'stodgy',\n",
       " 'sandra',\n",
       " 'equity',\n",
       " 'weekend',\n",
       " \"lloyd's\",\n",
       " 'saul',\n",
       " 'real',\n",
       " 'esselte',\n",
       " 'v-6',\n",
       " 'tony',\n",
       " 's.francisco',\n",
       " 'les',\n",
       " 'lancaster',\n",
       " 'poulin',\n",
       " 'hisseler/ortak',\n",
       " 'funding',\n",
       " 'service',\n",
       " 'kangyo',\n",
       " 'lipps',\n",
       " 'diet',\n",
       " 'tipasa',\n",
       " 'daremblum',\n",
       " \"arby's\",\n",
       " 'kurzweil',\n",
       " 'leveraged',\n",
       " 'dirks',\n",
       " 'leubert',\n",
       " 'erich',\n",
       " 'g-2',\n",
       " \"webster'e\",\n",
       " 'ahlerich',\n",
       " 'societa',\n",
       " \"westinghouse'un\",\n",
       " 'cincinati',\n",
       " 'qintex',\n",
       " \"bloomingdale's\",\n",
       " 'keith',\n",
       " 'dünya',\n",
       " 'mingo',\n",
       " 'dick',\n",
       " 'group',\n",
       " 'stovall',\n",
       " 'lang',\n",
       " 'research',\n",
       " 'centennial',\n",
       " 'old',\n",
       " 'sungard',\n",
       " \"donald'nın\",\n",
       " 'hawaii',\n",
       " 'beefeater',\n",
       " 'span',\n",
       " 'spaull',\n",
       " 'husker',\n",
       " 'yukon',\n",
       " 'pinpoint',\n",
       " 'ted',\n",
       " 'ftc',\n",
       " 'video',\n",
       " 'greenville',\n",
       " 'imelda',\n",
       " 'beacon',\n",
       " 'jones',\n",
       " 'cabrera',\n",
       " 'paul',\n",
       " 'dell',\n",
       " 'kabil',\n",
       " 'alfredo',\n",
       " 'pacemakers',\n",
       " 'shostakovich',\n",
       " 'selam',\n",
       " 'sierra',\n",
       " 'avusturalya',\n",
       " 'stanley',\n",
       " 'manhattan',\n",
       " 'hannifin',\n",
       " 'dobi',\n",
       " \"m'bow\",\n",
       " 'fda,',\n",
       " 'şirket',\n",
       " 'mip',\n",
       " 'scotch',\n",
       " \"corp.'daki\",\n",
       " 'hoover',\n",
       " 'cathcart',\n",
       " 'ağustos',\n",
       " 'batman',\n",
       " 'dur-kapat-sadece',\n",
       " 'bce',\n",
       " 'katz',\n",
       " 'teresa',\n",
       " 'stoll',\n",
       " 'texaco',\n",
       " 'kafka',\n",
       " 'kanadalı',\n",
       " \"corp.'da\",\n",
       " 'cynthia',\n",
       " 'waxman',\n",
       " 'alagoas',\n",
       " 'melloan',\n",
       " 'weekly',\n",
       " \"denny's\",\n",
       " 'cetus',\n",
       " \"iverson'ın\",\n",
       " 'kızıl',\n",
       " 'e-z',\n",
       " 'kern',\n",
       " 'einhorn',\n",
       " 'singapore',\n",
       " 'roseanne',\n",
       " 'gainen',\n",
       " 'martin',\n",
       " 'dillon',\n",
       " 'fanuc',\n",
       " 'bristol-myers',\n",
       " \"aids'in\",\n",
       " 'est,pbs',\n",
       " 'mastergate',\n",
       " 'network',\n",
       " 'skinner',\n",
       " 'little',\n",
       " 'bpc',\n",
       " 'mich',\n",
       " 'cumhuriyet',\n",
       " 'flesh',\n",
       " 'alf',\n",
       " 'vergi',\n",
       " 'no.',\n",
       " 'torrijos',\n",
       " 'tandy',\n",
       " 'massachusetts',\n",
       " \"weisfield's\",\n",
       " 'pont',\n",
       " 'trump',\n",
       " 'landscape',\n",
       " 'proleukin',\n",
       " 'okobank',\n",
       " 'dyer',\n",
       " 'kimbrough',\n",
       " 'ab',\n",
       " 'chin',\n",
       " 'malaise',\n",
       " 'endüstrisi',\n",
       " \"campeau'ın\",\n",
       " 'philips',\n",
       " 'beretta',\n",
       " 'kamboçya',\n",
       " 'patent',\n",
       " 'excalibur',\n",
       " 'miranda',\n",
       " 'mirage',\n",
       " 'belçika',\n",
       " 'peripherals',\n",
       " 'seltzer',\n",
       " 'baba',\n",
       " 'crest',\n",
       " 'nippon',\n",
       " 'lewis',\n",
       " 'tower',\n",
       " 'mücevherciler',\n",
       " 'bull',\n",
       " 'protection',\n",
       " 'johnson',\n",
       " 'politbüro',\n",
       " ...}"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon[\"PROPN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c63c583",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = []\n",
    "for sent in sentences:\n",
    "    for token in sent:\n",
    "        lemmas.append(token[\"lemma\"].lower())\n",
    "        \n",
    "lemmas = np.array(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "19688406",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = {}\n",
    "for key in lexicon.keys():\n",
    "    temp[key] = list(lexicon[key])\n",
    "\n",
    "with open(\"tr_syntactic_parser/grammar/lexicon.json\", \"w\", encoding='utf8') as f:\n",
    "    f.write(json.dumps([temp], ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "87f384b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ADJ',\n",
       " 'ADP',\n",
       " 'ADV',\n",
       " 'AUX',\n",
       " 'CCONJ',\n",
       " 'DET',\n",
       " 'INTJ',\n",
       " 'NOUN',\n",
       " 'NUM',\n",
       " 'PRON',\n",
       " 'PROPN',\n",
       " 'PUNCT',\n",
       " 'SCONJ',\n",
       " 'VERB',\n",
       " 'X']"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lex = json.loads(open(\"tr_syntactic_parser/grammar/lexicon.json\", \"r\", encoding='utf8').read())[0]\n",
    "sorted(list(lex.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9676897",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049d4b49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df79e72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e53903",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f69366b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zeyrek\n",
    "analyzer = zeyrek.MorphAnalyzer()\n",
    "\n",
    "def find_best_parse(word):\n",
    "    best_parse = None\n",
    "    max_freq = 0\n",
    "    for parsed in analyzer.analyze(word)[0]:\n",
    "        temp_freq = len(lemmas[parsed.lemma == lemmas])\n",
    "        if temp_freq > max_freq:\n",
    "            best_parse = parsed\n",
    "            max_freq = temp_freq\n",
    "        if parsed.pos == 'Verb':\n",
    "            best_parse = parsed\n",
    "            break\n",
    "            \n",
    "    return best_parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34cbf482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Parse(word='gitmek', lemma='gitmek', pos='Noun', morphemes=['Verb', 'Inf1', 'Noun', 'A3sg'], formatted='[gitmek:Verb] git:Verb|mek:Inf1→Noun+A3sg')]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.analyze('gitmek')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6661f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse(word='yazdım', lemma='yazmak', pos='Verb', morphemes=['Verb', 'Past', 'A1sg'], formatted='[yazmak:Verb] yaz:Verb+dı:Past+m:A1sg')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('yaz+dı+m', 'A1sg')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_parse = None\n",
    "max_freq = 0\n",
    "for parsed in analyzer.analyze('yazdım')[0]:\n",
    "    print(parsed)\n",
    "    temp_freq = len(lemmas[parsed.lemma == lemmas])\n",
    "    if temp_freq > max_freq:\n",
    "        best_parse = parsed\n",
    "        max_freq = temp_freq\n",
    "    if parsed.pos == 'Verb':\n",
    "        best_parse = parsed\n",
    "        break\n",
    "\n",
    "parts = best_parse.formatted.split(\" \")[1].split(\":\")\n",
    "parts = [part.split(\"+\")[-1] for part in parts]\n",
    "# parts[0] = best_parse.lemma\n",
    "analyzed = \"+\".join(parts[0:-1])\n",
    "word_type = parts[-1]\n",
    "analyzed, word_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "b1590a83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype='<U32')"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas[\"oynamak\"==lemmas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "08df03f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 14850/14850 [01:55<00:00, 128.73it/s]\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "fixes = {}\n",
    "    \n",
    "\n",
    "for sent in tqdm(sentences):\n",
    "    for token in sent:\n",
    "        word = token[\"form\"]\n",
    "        if token[\"upos\"] == \"VERB\":\n",
    "            \n",
    "            verb_possib = False\n",
    "            for parsed in analyzer.analyze(word)[0]:\n",
    "                temp_freq = len(lemmas[parsed.lemma == lemmas])\n",
    "                if not verb_possib:\n",
    "                    if temp_freq > max_freq:\n",
    "                        best_parse = parsed\n",
    "                        max_freq = temp_freq\n",
    "                    if parsed.pos == 'Verb':\n",
    "                        best_parse = parsed\n",
    "                        max_freq = temp_freq\n",
    "                else:\n",
    "                    if parsed.pos == 'Verb':\n",
    "                        if temp_freq > max_freq:\n",
    "                            best_parse = parsed\n",
    "                            max_freq = temp_freq\n",
    "\n",
    "            if best_parse == None: continue\n",
    "            else:            \n",
    "                for item in best_parse.formatted.split(\" \")[1].split(\"+\"):\n",
    "                    if \":\" in item:\n",
    "                        splitted = item.split(\":\")\n",
    "                        token = splitted[0]\n",
    "                        type_ = splitted[1].split(\"|\")[0]\n",
    "\n",
    "                        if type_ in fixes.keys(): fixes[type_].add(token)\n",
    "                        else: fixes[type_] = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "3e5eb3ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Verb', 'Prog1', 'Past', 'Noun', 'Acquire→Verb', 'Neg', 'P3sg', 'Cop', 'Adj', 'A3pl', 'Loc', 'A1sg', 'Unable', 'With→Adj', 'A2pl', 'Related→Adj', 'Aor', 'P2sg', 'Postp', 'Cond', 'A1pl', 'A2sg', 'Become→Verb', 'Pron', 'P3pl', 'Without→Adj', 'A3sg', 'Fut', 'Prog2', 'Agt→Noun', 'Neces', 'Narr', 'Ness→Noun', 'Acc', 'Abl', 'P1sg', 'Desr', 'Ins', 'Dat', 'Num', 'Opt', 'Gen', 'Interj', 'P1pl', 'Equ', 'P2pl', 'Conj'])"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12249ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"A1pl\",\n",
    "    \"A1sg\",\n",
    "    \"A2pl\",\n",
    "    \"A2sg\",\n",
    "    \"A3pl\",\n",
    "    \"A3sg\",\n",
    "    \"Dat\" : \"DATIVE\",\n",
    "    \"Fut\",\n",
    "    \"P1pl\",\n",
    "    \"P1sg\",\n",
    "    \"P2pl\",\n",
    "    \"P2sg\",\n",
    "    \"P3pl\",\n",
    "    \"P3sg\",\n",
    "    \"Past\" : \"PAST\",\n",
    "    \"Prog1\",\n",
    "    \"Prog2\",\n",
    "    \"Cop\" : \"PREPARTICLE\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "a147c55e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yeceğyım\n",
      "yeceğyim\n",
      "yeceğum\n",
      "yeceğim\n",
      "yeceğüm\n",
      "yeceğyum\n",
      "yeceğm\n",
      "yeceğım\n",
      "yecekyım\n",
      "yecekyim\n",
      "yecekum\n",
      "yecekim\n",
      "yeceküm\n",
      "yecekyum\n",
      "yecekm\n",
      "yecekım\n",
      "yacakyım\n",
      "yacakyim\n",
      "yacakum\n",
      "yacakim\n",
      "yacaküm\n",
      "yacakyum\n",
      "yacakm\n",
      "yacakım\n",
      "eceğyım\n",
      "eceğyim\n",
      "eceğum\n",
      "eceğim\n",
      "eceğüm\n",
      "eceğyum\n",
      "eceğm\n",
      "eceğım\n",
      "acağyım\n",
      "acağyim\n",
      "acağum\n",
      "acağim\n",
      "acağüm\n",
      "acağyum\n",
      "acağm\n",
      "acağım\n",
      "ecekyım\n",
      "ecekyim\n",
      "ecekum\n",
      "ecekim\n",
      "eceküm\n",
      "ecekyum\n",
      "ecekm\n",
      "ecekım\n",
      "acakyım\n",
      "acakyim\n",
      "acakum\n",
      "acakim\n",
      "acaküm\n",
      "acakyum\n",
      "acakm\n",
      "acakım\n",
      "yacağyım\n",
      "yacağyim\n",
      "yacağum\n",
      "yacağim\n",
      "yacağüm\n",
      "yacağyum\n",
      "yacağm\n",
      "yacağım\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "\n",
    "import itertools\n",
    "a = list(fixes[\"Fut\"])\n",
    "b = list(fixes[\"A1sg\"])\n",
    "suffixes = []\n",
    "for pair in list(itertools.product(a, b)):\n",
    "    print(pair[0] + pair[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "92132a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'yım', 'yim', 'um', 'im', 'üm', 'yum', 'm', 'ım'}\n",
      "{'ım', 'im', 'm'}\n"
     ]
    }
   ],
   "source": [
    "print(fixes[\"A1sg\"])\n",
    "print(fixes[\"P1sg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "839fc836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "00e99381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A1pl \t 11\n",
      "A1sg \t 8\n",
      "A2pl \t 11\n",
      "A2sg \t 5\n",
      "A3pl \t 5\n",
      "A3sg \t 3\n",
      "Abl \t 4\n",
      "Acc \t 1\n",
      "Acquire→Verb \t 2\n",
      "Adj \t 154\n",
      "Agt→Noun \t 5\n",
      "Aor \t 8\n",
      "Become→Verb \t 2\n",
      "Cond \t 4\n",
      "Conj \t 1\n",
      "Cop \t 8\n",
      "Dat \t 5\n",
      "Desr \t 2\n",
      "Equ \t 1\n",
      "Fut \t 8\n",
      "Gen \t 3\n",
      "Ins \t 4\n",
      "Interj \t 1\n",
      "Loc \t 6\n",
      "Narr \t 6\n",
      "Neces \t 2\n",
      "Neg \t 3\n",
      "Ness→Noun \t 4\n",
      "Noun \t 649\n",
      "Num \t 7\n",
      "Opt \t 4\n",
      "P1pl \t 1\n",
      "P1sg \t 3\n",
      "P2pl \t 1\n",
      "P2sg \t 5\n",
      "P3pl \t 3\n",
      "P3sg \t 7\n",
      "Past \t 12\n",
      "Postp \t 4\n",
      "Prog1 \t 5\n",
      "Prog2 \t 2\n",
      "Pron \t 7\n",
      "Related→Adj \t 2\n",
      "Unable \t 5\n",
      "Verb \t 637\n",
      "Without→Adj \t 4\n",
      "With→Adj \t 4\n"
     ]
    }
   ],
   "source": [
    "for item in dict(sorted(fixes.items())).items():\n",
    "    print(item[0],\"\\t\",len(item[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "37e032e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ben', 'A3sg']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[part.split(\":\")[0] for part in analyzer.analyze('ben')[0][0].formatted.split(\" \")[-1].split(\"+\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "1bf72292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Parse(word='geleceksin', lemma='gelecek', pos='Verb', morphemes=['Adj', 'Zero', 'Verb', 'Pres', 'A2sg'], formatted='[gelecek:Adj] gelecek:Adj|Zero→Verb+Pres+sin:A2sg'),\n",
       "  Parse(word='geleceksin', lemma='gelecek', pos='Verb', morphemes=['Noun', 'A3sg', 'Zero', 'Verb', 'Pres', 'A2sg'], formatted='[gelecek:Noun] gelecek:Noun+A3sg|Zero→Verb+Pres+sin:A2sg')]]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.analyze(\"geleceksin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "cfc17e17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gelecek', 'sin']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[part.split(\":\")[0] for part in list(filter(lambda x: \":\" in x, analyzer.analyze(\"geleceksin\")[0][0].formatted.split(\" \")[-1].split(\"+\")))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "4dc80cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 14850/14850 [18:02<00:00, 13.71it/s]\n"
     ]
    }
   ],
   "source": [
    "word = \"gelmişti\"\n",
    "\n",
    "merged_types = {}\n",
    "for sent in tqdm(sentences):\n",
    "    for token in sent:\n",
    "        try:\n",
    "            word = token[\"form\"]\n",
    "            best_parse = find_best_parse(word)\n",
    "            if best_parse:\n",
    "                text = [part.split(\":\")[0] for part in list(filter(lambda x: \":\" in x, best_parse.formatted.split(\" \")[-1].split(\"+\")))]\n",
    "                types = [part.split(\":\")[-1] for part in list(filter(lambda x: \":\" in x, best_parse.formatted.split(\" \")[-1].split(\"+\")))]\n",
    "                \n",
    "                merged_type = \"\".join(types[1:])\n",
    "                if \"→\" in merged_type or \"|\" in merged_type: continue\n",
    "                text, types\n",
    "                if merged_type in merged_types.keys(): merged_types[merged_type].add(\"\".join(text[1:]))\n",
    "                else: merged_types[merged_type] = set([\"\".join(text[1:])])\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "b4b76667",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['', 'Prog1', 'A3plLoc', 'Past', 'Loc', 'P3sgGen', 'P2sgAcc', 'P3sgAbl', 'P3sgDat', 'Acc', 'NegPast', 'Dat', 'Abl', 'A3plDat', 'NegProg1', 'P3pl', 'P1sgAbl', 'A3plP3pl', 'P2sgGen', 'P3sgAcc', 'A3pl', 'A2pl', 'Gen', 'Opt', 'A2sg', 'Narr', 'Cop', 'Aor', 'A3plGen', 'Neg', 'P3sgLoc', 'Ins', 'PastA3pl', 'P3plGen', 'P3plDat', 'Prog1A3pl', 'P3sg', 'Prog2Cop', 'P1sgAcc', 'PastA1sg', 'UnablePast', 'A3plAcc', 'AorCop', 'P2sgDat', 'P2sgLoc', 'P2sgEqu', 'NegA2pl', 'NegOpt', 'A1sg', 'A3plP2sgLoc', 'P3plLoc', 'A3sg', 'A3plP2sgAcc', 'Cond', 'CondA1pl', 'P3sgIns', 'P2sgAbl', 'P1pl', 'PastA2sg', 'P3plAcc', 'Prog1A1sg', 'NegProg1A1sg', 'Prog1A1pl', 'P1sgDat', 'Prog1Past', 'A1pl', 'NegProg1Past', 'P1sg', 'P3plAbl', 'A3plAbl', 'NegPastA3pl', 'UnablePastA1pl', 'AorPast', 'P1plGen', 'P2sg', 'NegProg1A1pl', 'P3plIns', 'NegNecesPast', 'A3plIns', 'AorA2sg', 'NegA3sg', 'NecesCop', 'NegFut', 'Prog2A3pl', 'NarrPastA1sg', 'AorA1pl', 'OptA1pl', 'AorA3pl', 'Neces', 'A3plP2sgDat', 'Prog2Past', 'A3plEqu', 'NegAor', 'PastA1pl', 'NegNarr', 'NegAorA2pl', 'FutA1pl', 'NegAorCond', 'NegPastA1pl', 'NegProg1A3pl', 'P1sgLoc', 'P2plLoc', 'Desr', 'NegProg1Cond', 'NegAorCop', 'NarrCop', 'Equ', 'NegFutA3pl', 'A3plP2sgAbl', 'A3plP2sgGen', 'Prog1PastA1sg', 'NarrPast', 'Prog1A2sg', 'P1plLoc', 'Prog2', 'A3plP3plAcc', 'P1plAcc', 'FutPast', 'A3plP1sgAcc', 'NegPastA1sg', 'A3plP1plDat', 'AorA3plCond', 'DesrA1sg', 'P1sgGen', 'OptPast', 'FutA1sg', 'A3plP1pl', 'NecesA1pl', 'OptA1sg', 'NecesPast', 'AorA1sg', 'DesrPast', 'A3plCop', 'A3plP3plGen', 'NecesPastA3pl', 'UnableAor', 'P3sgEqu', 'NegAorPast', 'DesrA1pl', 'AorCond', 'A1sgCop', 'Unable', 'A3plP1plGen', 'Prog2CopA3pl', 'AorA2pl', 'NegA1sg', 'A3plP3sgIns', 'Fut', 'A3plP3plDat', 'NecesA3pl', 'A3plP1plAcc', 'Prog1A3plPast', 'P2plGen', 'P1sgEqu', 'NegAorCondA2sg', 'NegNeces', 'UnablePastA3pl', 'UnableAorCond', 'NegNarrCop', 'NegProg1PastA1sg', 'A3plPast', 'PastA2pl', 'NegFutA1sg', 'CondA2sg', 'NegNarrPast', 'NegFutA1pl', 'FutCop', 'NegPastA2sg', 'NegDesrPast', 'A3plP3sg', 'CondA2pl', 'P2plDat', 'NarrPastA1pl', 'UnableProg1', 'DesrA2sg', 'PastCond', 'NarrA3plPast', 'NegNarrPastA1sg', 'OptA3pl', 'UnableAorPast', 'UnableProg1A1pl', 'NegFutCop', 'CondA3pl', 'P1plDat', 'NegProg2Cop', 'NegDesr', 'AorPastA1sg', 'PastCondA2sg', 'A3plP1plLoc', 'NegNecesA1pl', 'NegFutPast', 'Prog1CondA2sg', 'NarrA3pl', 'CopA3pl', 'NegNecesCop', 'NegProg1A2sg', 'NegAorCondA1pl', 'NarrA3plCop', 'NegAorA3plPast', 'FutA3plCop', 'NegPastCond', 'P2plAcc', 'NegDesrA3pl', 'A3plP1sgDat', 'A3plP2plAbl', 'Prog1PastA2sg', 'DesrA3pl', 'NegA1pl', 'NecesA1sg', 'DesrPastA1sg', 'Prog1A2pl', 'NegAorA3pl', 'NegAorPastA1sg', 'A3plP1plAbl', 'NarrPastA3pl', 'P1sgIns', 'A3plP2plIns', 'Prog1PastA1pl', 'Prog2A3plPast', 'A3plP1sgIns', 'UnableAorA2sg', 'DesrA2pl', 'DesrPastA1pl', 'NegAorA2sg', 'A3plP2plGen', 'Prog1Cond', 'Prog1Cop', 'DesrA3plPast', 'NecesA2pl', 'AorCondA2pl', 'PastCondA1sg', 'P2sgIns', 'NarrCond', 'UnableOpt', 'Prog1CondA1pl', 'A3plP2plEqu', 'NegFutA3plCond', 'UnableProg1A1sg', 'FutCond', 'AorPastA1pl', 'P1plIns', 'NegFutCond', 'NegOptA1pl', 'NegProg1A3plPast', 'NegA3pl', 'NegProg1CondA1pl', 'FutA3pl', 'A3plP2plAcc', 'NegDesrPastA1pl', 'UnableAorA3pl', 'A3plP3sgGen', 'NegProg1A2pl', 'A3plP1plIns', 'CondA1sg', 'NegFutNarr', 'NecesA2sg', 'Prog1Narr', 'UnableAorA2pl', 'NarrA1sg', 'P2plAbl', 'UnableFutCop'])"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_types.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "8ad35779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NegFutA3pl', 'FutA3plCop', 'NegFutA3plCond', 'FutA3pl'],\n",
       "      dtype='<U16')"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = np.array(list(merged_types.keys()))\n",
    "keys[[True if \"FutA3\" in key else False for key in keys]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "eea900d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'meyeceklerse'}"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_types[\"NegFutA3plCond\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "3cbe26da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mazdı']"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[\"NegAorPast\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "d358ccaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "temp = {}\n",
    "for key in merged_types.keys():\n",
    "    if \"Prog\" in key or \"Fut\" in key or \"Past\" in key:\n",
    "        temp[key] = list(merged_types[key])\n",
    "with open('data/merged_types.json', 'w') as f:\n",
    "    json.dump(temp, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "82fa4c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "65478108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRE1 -> yor\n",
      "PRE1 -> ıyor\n",
      "PRE1 -> üyor\n",
      "PRE1 -> uyor\n",
      "PRE1 -> iyor\n",
      "PAST3 -> tu\n",
      "PAST3 -> tı\n",
      "PAST3 -> tü\n",
      "PAST3 -> dü\n",
      "PAST3 -> ydü\n",
      "PAST3 -> ydu\n",
      "PAST3 -> du\n",
      "PAST3 -> ydi\n",
      "PAST3 -> di\n",
      "PAST3 -> ti\n",
      "PAST3 -> ydı\n",
      "PAST3 -> dı\n",
      "PAST3 -> medi\n",
      "PAST3 -> madı\n",
      "PRE1 -> muyor\n",
      "PRE1 -> müyor\n",
      "PRE1 -> mıyor\n",
      "PRE1 -> miyor\n",
      "PAST3 -> dılar\n",
      "PAST3 -> tiler\n",
      "PAST3 -> tular\n",
      "PAST3 -> tüler\n",
      "PAST3 -> düler\n",
      "PAST3 -> tılar\n",
      "PAST3 -> dular\n",
      "PAST3 -> diler\n",
      "PRE1 -> üyorlar\n",
      "PRE1 -> uyorlar\n",
      "PRE1 -> yorlar\n",
      "PRE1 -> iyorlar\n",
      "PRE1 -> ıyorlar\n",
      "PRE2 -> mektedir\n",
      "PRE2 -> maktadır\n",
      "PAST1 -> dum\n",
      "PAST1 -> dim\n",
      "PAST1 -> tum\n",
      "PAST1 -> dım\n",
      "PAST1 -> düm\n",
      "PAST1 -> tım\n",
      "PAST1 -> tim\n",
      "PAST3 -> amadı\n",
      "PAST3 -> emedi\n",
      "PAST2 -> din\n",
      "PAST2 -> dın\n",
      "PAST2 -> dun\n",
      "PAST2 -> tun\n",
      "PAST2 -> tin\n",
      "PRE1 -> ıyorum\n",
      "PRE1 -> uyorum\n",
      "PRE1 -> üyorum\n",
      "PRE1 -> iyorum\n",
      "PRE1 -> yorum\n",
      "PRE1 -> müyorum\n",
      "PRE1 -> mıyorum\n",
      "PRE1 -> miyorum\n",
      "PRE1 -> muyorum\n",
      "PRE1 -> ıyoruz\n",
      "PRE1 -> iyoruz\n",
      "PRE1 -> yoruz\n",
      "PRE1 -> uyoruz\n",
      "PRE1 -> üyoruz\n",
      "PRE1 -> uyordu\n",
      "PRE1 -> ıyordu\n",
      "PRE1 -> üyordu\n",
      "PRE1 -> yordu\n",
      "PRE1 -> iyordu\n",
      "PRE1 -> mıyordu\n",
      "PRE1 -> muyordu\n",
      "PRE1 -> miyordu\n",
      "PRE1 -> müyordu\n",
      "PAST3 -> madılar\n",
      "PAST3 -> mediler\n",
      "PAST1 -> amadık\n",
      "PAST3 -> rdu\n",
      "PAST3 -> ürdü\n",
      "PAST3 -> ardı\n",
      "PAST3 -> erdi\n",
      "PAST3 -> rdı\n",
      "PAST3 -> rdi\n",
      "PAST3 -> ırdı\n",
      "PAST3 -> irdi\n",
      "PRE1 -> miyoruz\n",
      "PRE1 -> müyoruz\n",
      "PRE1 -> mıyoruz\n",
      "PRE1 -> muyoruz\n",
      "PAST3 -> mamalıydı\n",
      "FUT3 -> meyecek\n",
      "FUT3 -> mayacak\n",
      "PRE2 -> mekteler\n",
      "PRE2 -> maktalar\n",
      "PAST1 -> miştim\n",
      "PAST1 -> mıştım\n",
      "PAST1 -> muştum\n",
      "PRE2 -> maktaydı\n",
      "PRE2 -> mekteydi\n",
      "PAST1 -> tuk\n",
      "PAST1 -> dik\n",
      "PAST1 -> tik\n",
      "PAST1 -> dık\n",
      "PAST1 -> duk\n",
      "PAST1 -> tık\n",
      "PAST1 -> dük\n",
      "FUT1 -> eceğiz\n",
      "PAST1 -> medik\n",
      "PAST1 -> madık\n",
      "PRE1 -> mıyorlar\n",
      "PRE1 -> muyorlar\n",
      "PRE1 -> miyorlar\n",
      "PRE1 -> müyorsa\n",
      "PRE1 -> mıyorsa\n",
      "FUT3 -> mayacaklar\n",
      "FUT3 -> meyecekler\n",
      "PRE1 -> ıyordum\n",
      "PRE1 -> iyordum\n",
      "PAST3 -> mişti\n",
      "PAST3 -> mıştı\n",
      "PAST3 -> müştü\n",
      "PAST3 -> muştu\n",
      "PRE1 -> ıyorsun\n",
      "PRE1 -> iyorsun\n",
      "PRE2 -> makta\n",
      "PRE2 -> mekte\n",
      "FUT3 -> ecekti\n",
      "FUT3 -> acaktı\n",
      "PAST1 -> madım\n",
      "PAST1 -> medim\n",
      "PAST3 -> aydı\n",
      "FUT1 -> yacağım\n",
      "FUT1 -> acağım\n",
      "FUT1 -> eceğim\n",
      "PAST3 -> malıydı\n",
      "PAST3 -> meliydi\n",
      "PAST3 -> saydı\n",
      "PAST3 -> seydi\n",
      "PAST3 -> malıydılar\n",
      "PAST3 -> mazdı\n",
      "PRE2 -> maktadırlar\n",
      "PRE2 -> mektedirler\n",
      "FUT3 -> yecek\n",
      "PRE1 -> uyorlardı\n",
      "PRE1 -> üyorlardı\n",
      "PRE1 -> iyorlardı\n",
      "PRE1 -> ıyorlardı\n",
      "PAST3 -> amadılar\n",
      "PAST3 -> emediler\n",
      "PRE1 -> mıyordum\n",
      "PRE1 -> miyordum\n",
      "PAST3 -> sinlerdi\n",
      "PAST2 -> tünüz\n",
      "PAST2 -> diniz\n",
      "PAST2 -> dınız\n",
      "PAST2 -> dunuz\n",
      "FUT1 -> mayacağım\n",
      "FUT1 -> meyeceğim\n",
      "PAST3 -> memişti\n",
      "PAST3 -> mamıştı\n",
      "FUT1 -> mayacağız\n",
      "FUT1 -> meyeceğiz\n",
      "FUT3 -> ecektir\n",
      "FUT3 -> yecektir\n",
      "FUT3 -> acaktır\n",
      "PAST2 -> medin\n",
      "PAST3 -> masaydı\n",
      "PAST1 -> mıştık\n",
      "PAST1 -> miştik\n",
      "PRE1 -> emiyor\n",
      "PRE1 -> amıyor\n",
      "PRE1 -> yamıyor\n",
      "PAST3 -> dıysa\n",
      "PAST3 -> mişlerdi\n",
      "PAST3 -> müşlerdi\n",
      "PAST3 -> mışlardı\n",
      "PAST1 -> memiştim\n",
      "PAST1 -> mamıştım\n",
      "PAST3 -> amazdı\n",
      "PAST3 -> emezdi\n",
      "PRE1 -> emiyoruz\n",
      "FUT3 -> mayacaktır\n",
      "FUT3 -> meyecektir\n",
      "PRE2 -> mamaktadır\n",
      "PRE2 -> memektedir\n",
      "PAST1 -> erdim\n",
      "PAST2 -> duysan\n",
      "FUT3 -> mayacaktı\n",
      "PRE1 -> iyorsan\n",
      "PRE1 -> miyorsun\n",
      "PRE1 -> mıyorsun\n",
      "PRE1 -> müyorsun\n",
      "PAST3 -> mazlardı\n",
      "FUT3 -> acaklardır\n",
      "PAST3 -> mediyse\n",
      "PRE1 -> iyordun\n",
      "PAST1 -> saydım\n",
      "PRE1 -> iyorsunuz\n",
      "PRE1 -> yorsunuz\n",
      "PRE1 -> üyorsunuz\n",
      "PRE1 -> ıyorsunuz\n",
      "PAST1 -> mezdim\n",
      "PAST3 -> muştular\n",
      "PRE1 -> iyorduk\n",
      "PRE2 -> mektelerdi\n",
      "PAST1 -> saydık\n",
      "PRE1 -> iyorsa\n",
      "PRE1 -> ıyorsa\n",
      "PRE1 -> ıyordur\n",
      "PRE1 -> üyordur\n",
      "PAST3 -> salardı\n",
      "PAST3 -> selerdi\n",
      "PAST1 -> diysem\n",
      "PRE1 -> iyorsak\n",
      "FUT3 -> meyeceklerse\n",
      "PRE1 -> yamıyorum\n",
      "FUT3 -> acaksa\n",
      "PAST1 -> ırdık\n",
      "PAST1 -> irdik\n",
      "FUT3 -> meyecekse\n",
      "PRE1 -> miyorlardı\n",
      "PRE1 -> miyorsak\n",
      "FUT3 -> ecekler\n",
      "PAST1 -> masaydık\n",
      "PRE1 -> müyorsunuz\n",
      "PRE1 -> muyorsunuz\n",
      "FUT3 -> mayacakmış\n",
      "PRE1 -> ıyormuş\n",
      "FUT3 -> amayacaktır\n"
     ]
    }
   ],
   "source": [
    "for key in temp.keys():\n",
    "    if \"Past\" in key: tense = \"PAST\"\n",
    "    if \"Prog\" in key: tense = \"PRE\"\n",
    "    if \"Fut\" in key: tense = \"FUT\"\n",
    "        \n",
    "        \n",
    "    if \"1\" in key: finalkey = tense+\"1\"\n",
    "    elif \"2\" in key: finalkey = tense+\"2\"\n",
    "    elif \"3\" in key: finalkey = tense+\"3\"\n",
    "    else: finalkey = tense+\"3\"\n",
    "        \n",
    "    for suff in temp[key]:\n",
    "        print(finalkey,\"->\",suff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "ce3806c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = json.loads(open(\"tr_syntactic_parser/grammar/all_suffix_classes.json\", \"r\", encoding=\"utf-8\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "060aecde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A1pl \t 10\n",
      "A1sg \t 8\n",
      "A2pl \t 9\n",
      "A2sg \t 4\n",
      "A3pl \t 2\n",
      "A3sg \t 0\n",
      "Abl \t 3\n",
      "Acc \t 1\n",
      "Acquire→Verb \t 2\n",
      "Adj \t 178\n",
      "Adv \t 0\n",
      "Agt→Noun \t 4\n",
      "Aor \t 3\n",
      "Become→Verb \t 2\n",
      "Cond \t 4\n",
      "Conj \t 2\n",
      "Cop \t 8\n",
      "Dat \t 5\n",
      "Desr \t 2\n",
      "Equ \t 0\n",
      "Fut \t 5\n",
      "Gen \t 4\n",
      "Ins \t 3\n",
      "Interj \t 1\n",
      "Loc \t 6\n",
      "Narr \t 5\n",
      "Neces \t 2\n",
      "Neg \t 3\n",
      "Ness→Noun \t 4\n",
      "Noun \t 551\n",
      "Num \t 7\n",
      "Opt \t 2\n",
      "P1pl \t 1\n",
      "P1sg \t 2\n",
      "P2pl \t 0\n",
      "P2sg \t 5\n",
      "P3pl \t 3\n",
      "P3sg \t 7\n",
      "Past \t 12\n",
      "Postp \t 5\n",
      "Prog1 \t 3\n",
      "Prog2 \t 0\n",
      "Pron \t 5\n",
      "Related→Adj \t 2\n",
      "Unable \t 2\n",
      "Verb \t 4\n",
      "Without→Adj \t 4\n",
      "With→Adj \t 4\n"
     ]
    }
   ],
   "source": [
    "for item in dict(sorted(classes.items())).items():\n",
    "    print(item[0],\"\\t\",len(item[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d36bd0e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Parse(word='dinleme', lemma='dinlemek', pos='Verb', morphemes=['Verb', 'Neg', 'Imp', 'A2sg'], formatted='[dinlemek:Verb] dinle:Verb+me:Neg+Imp+A2sg'),\n",
       "  Parse(word='dinleme', lemma='dinlemek', pos='Noun', morphemes=['Verb', 'Inf2', 'Noun', 'A3sg'], formatted='[dinlemek:Verb] dinle:Verb|me:Inf2→Noun+A3sg')]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.analyze(\"dinleme\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
