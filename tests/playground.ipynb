{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_raw = open('data/corpus/UD_Turkish-BOUN/tr_boun-ud-train.conllu', 'r', encoding=\"utf-8\").read()\n",
    "data_raw = open('data/corpus/UD_Turkish-Penn/tr_penn-ud-train.conllu', 'r', encoding=\"utf-8\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = conllu.parse(data_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "upos = []\n",
    "for sent in sentences:\n",
    "    for token in sent:\n",
    "        tokens.append(token[\"form\"])\n",
    "        upos.append(token[\"upos\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = {}\n",
    "stems = set()\n",
    "for sent in sentences:\n",
    "    for token in sent:\n",
    "        upos = token[\"upos\"]\n",
    "        if upos in lexicon.keys(): lexicon[upos].add(token[\"lemma\"].lower())\n",
    "        else: lexicon[upos] = set([token[\"lemma\"].lower()])\n",
    "        stems.add(token[\"lemma\"].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(166514, 166514)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens), len(upos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ADJ',\n",
       " 'ADP',\n",
       " 'ADV',\n",
       " 'AUX',\n",
       " 'CCONJ',\n",
       " 'DET',\n",
       " 'INTJ',\n",
       " 'NOUN',\n",
       " 'NUM',\n",
       " 'PRON',\n",
       " 'PROPN',\n",
       " 'PUNCT',\n",
       " 'SCONJ',\n",
       " 'VERB',\n",
       " 'X'}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(upos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 \t-\t Bayan\n",
      "4 \t-\t Haag\n",
      "4 \t-\t Elianti\n",
      "0 \t-\t oynuyor\n",
      "4 \t-\t .\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sentences[0])):\n",
    "    print(sentences[0][i][\"head\"], \"\\t-\\t\", sentences[0][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TokenList<Bayan, Haag, Elianti, oynuyor, ., metadata={sent_id: \"15-0000.train\", text: \"Bayan Haag Elianti oynuyor .\"}>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1,\n",
       " 'form': 'Bayan',\n",
       " 'lemma': 'bayan',\n",
       " 'upos': 'NOUN',\n",
       " 'xpos': None,\n",
       " 'feats': {'Case': 'Nom', 'Number': 'Sing', 'Person': '3'},\n",
       " 'head': 2,\n",
       " 'deprel': 'nmod',\n",
       " 'deps': None,\n",
       " 'misc': None}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 2,\n",
       " 'form': 'Haag',\n",
       " 'lemma': 'haag',\n",
       " 'upos': 'PROPN',\n",
       " 'xpos': None,\n",
       " 'feats': {'Case': 'Nom', 'Number': 'Sing'},\n",
       " 'head': 4,\n",
       " 'deprel': 'nsubj',\n",
       " 'deps': None,\n",
       " 'misc': None}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 3,\n",
       " 'form': 'Elianti',\n",
       " 'lemma': 'elianti',\n",
       " 'upos': 'PROPN',\n",
       " 'xpos': None,\n",
       " 'feats': {'Case': 'Nom', 'Number': 'Sing'},\n",
       " 'head': 4,\n",
       " 'deprel': 'obj',\n",
       " 'deps': None,\n",
       " 'misc': None}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 4,\n",
       " 'form': 'oynuyor',\n",
       " 'lemma': 'oyna',\n",
       " 'upos': 'VERB',\n",
       " 'xpos': None,\n",
       " 'feats': {'Aspect': 'Prog',\n",
       "  'Mood': 'Ind',\n",
       "  'Number': 'Sing',\n",
       "  'Person': '3',\n",
       "  'Polarity': 'Pos',\n",
       "  'Tense': 'Pres',\n",
       "  'VerbForm': 'Fin'},\n",
       " 'head': 0,\n",
       " 'deprel': 'root',\n",
       " 'deps': None,\n",
       " 'misc': None}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zeyrek\n",
    "analyzer = zeyrek.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ban+a', 'A3sg')"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parts = analyzer.analyze('bana')[0][0].formatted.split(\" \")[1].split(\":\")\n",
    "parts = [part.split(\"+\")[-1] for part in parts]\n",
    "analyzed = \"+\".join(parts[0:-1])\n",
    "word_type = parts[-1]\n",
    "analyzed, word_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Parse(word='bana', lemma='banmak', pos='Verb', morphemes=['Verb', 'Opt', 'A3sg'], formatted='[banmak:Verb] ban:Verb+a:Opt+A3sg'),\n",
       "  Parse(word='bana', lemma='Ba', pos='Noun', morphemes=['Noun', 'A3sg', 'P2sg', 'Dat'], formatted='[Ba:Noun,Prop] ba:Noun+A3sg+n:P2sg+a:Dat'),\n",
       "  Parse(word='bana', lemma='ban', pos='Noun', morphemes=['Noun', 'A3sg', 'Dat'], formatted='[ban:Noun] ban:Noun+A3sg+a:Dat'),\n",
       "  Parse(word='bana', lemma='ben', pos='Pron', morphemes=['Pron', 'A1sg', 'Dat'], formatted='[ben:Pron,Pers] ban:Pron+A1sg+a:Dat')]]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.analyze('bana')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = np.array([])\n",
    "for sent in sentences:\n",
    "    for token in sent:\n",
    "        lemmas = np.append(lemmas, token[\"lemma\"].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
